input {
   file {
      type => "log4j"
#      path => "/home/afield/Documents/Map-Reduce/Jenkins Jobs/64 nodes Hyperion/JDG64vs63vsISPN7/JDG64GA/archive/report/stdout/*.log"
#      path => "/home/afield/Documents/Map-Reduce/Jenkins Jobs/64 nodes Hyperion/JDG64vs63vsISPN7/JDG64GA/archive/report/stdout/node1.log"
      # 5GB of logs
#      path => "/home/afield/Documents/JDG 6.3/resilience/42/stdout/*.log"

#      path => "/home/afield/Documents/JDG_6.5/cs resilience/74/zip/report/stdout/*.log"
      
#      path => "/home/afield/Documents/Logstash/MatejLogs/logs/*.log"
      path => "/home/afield/Documents/Logstash/MatejLogs/logs/edg-perf02.log"

#       sincedb_path => "/dev/null"                                       

      # Re-parse the files
      start_position => beginning
      
      # If the line begins with a number or the string "Thread", then parse it
      # Otherwise, it is part of a multiline log statement
      # (The "Thread" messages are log output from the mappers)
      codec => multiline {
         pattern => "^(?<no_multi>Thread|%{NUMBER})"
         negate => "true"
         what => "previous"
      }
   }
}

filter {
   ruby {
      # Get today's date minus one day (Can't add events that happen in the future!)
      code => 'event["datestamp"]=(Date.today - 1)'
   }
   
   grok {
      # Parse RadarGun log messages
      # 14:36:03,462 WARN  [org.radargun.utils.ArgsHolder] (main) Switch -master is deprecated. Use --master instead.
      # %d{HH:mm:ss,SSS} %-5p [%c] (%t) %m%n
      match => [ "message", "%{DATA:log_timestamp}%{SPACE}%{LOGLEVEL:level}%{SPACE}[\[]%{DATA:category}[\]]%{SPACE}[(]%{DATA:thread}[)]%{SPACE}%{GREEDYDATA:summary}" ]

      add_tag => [ "RadarGun" ]
      add_field => { "date_time" => "%{datestamp} %{log_timestamp}" }
   }
   
   if "_grokparsefailure" in [tags] {
      grok {
         # Parse Java GC log messages TODO: Get more detail, esp the real time
         # 2015-01-27T14:36:14.128-0800: [Full GC2015-01-27T14:36:14.128-0800: [CMS: 7947K->15332K(3145728K), 0.1400580 secs] 214968K->15332K(4089472K), [CMS Perm : 27445K->27439K(27636K)], 0.1402000 secs] [Times: user=0.15 sys=0.00, real=0.14 secs] 
         # 2015-01-27T14:36:18.181-0800: [GC2015-01-27T14:36:18.182-0800: [ParNew: 838912K->104831K(943744K), 0.1146040 secs] 854244K->305438K(4089472K), 0.1146790 secs] [Times: user=0.24 sys=0.22, real=0.11 secs] 
         remove_tag => ["_grokparsefailure"]
         match => [ "message", "%{YEAR}-%{MONTHNUM}-%{MONTHDAY}T%{HOUR:hour}:%{MINUTE:minute}:%{SECOND:second}%{ISO8601_TIMEZONE}:%{SPACE}%{GREEDYDATA:summary} \[Times: user=%{NUMBER:user:float} sys=%{NUMBER:sys:float}, real=%{NUMBER:real:float} secs\]" ]
         add_field => { "date_time" => "%{datestamp} %{hour}:%{minute}:%{second}" }
         add_tag => [ "GCLog" ]
      }
      if [summary] {
         grok {
            # Add tag to "Full GC" log messages
            match => [ "summary", "(?<fullgc>Full GC)" ]
            add_tag => [ "FullGC" ]
            
            # Don't flag if match fails
            tag_on_failure =>  []
         }
      }
   }
      
   date {
      # Parse the dates
      locale => "en"
      timezone => "UTC"
      match => ["date_time", "yyyy-MM-dd HH:mm:ss','SSS", "yyyy-MM-dd HH:mm:ss'.'SSS" ]
   }
   
   grok {
      # Get the filename without the file extension
      match => [ "path", "%{GREEDYDATA}/%{GREEDYDATA:filename}\.log" ]
   }
   
   mutate {
      # Use the filename as the host name
      replace => [ "host", "%{filename}" ]

      # Remove temporary fields
      remove_field => [ 
                         "datestamp", 
                         "log_timestamp", 
                         "date_time", 
                         "hour", 
                         "minute", 
                         "second",
                         "fullgc"
                      ]
   }
   
   # Delete events that grok can't parse
   if "_grokparsefailure" in [tags] {
      drop {}
   }
}

output {
  elasticsearch {
    # Setting 'embedded' will run  a real elasticsearch server inside logstash.
    # This option below saves you from having to run a separate process just
    # for ElasticSearch, so you can get started quicker!
    embedded => true
    workers => 4
  }
  stdout { codec => rubydebug }
}
